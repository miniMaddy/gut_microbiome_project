{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88aa52b9",
   "metadata": {},
   "source": [
    "# DIABIMMUNE Microbiome Data Preprocessing\n",
    "\n",
    "This notebook preprocesses microbiome data from the **DIABIMMUNE** study for **early prediction of food allergies** (milk, egg, peanut).\n",
    "\n",
    "## Key Challenges Solved\n",
    "\n",
    "1. **SRS → Sample Linking**: Public SRS (Sequence Read Archive Sample) IDs from MicrobeAtlas need to be linked to internal study metadata, but there's no direct mapping column.\n",
    "\n",
    "2. **Timepoint Recovery**: The public SRS data doesn't include `collection_month`, which is critical for longitudinal analysis. We recover this by querying the ENA (European Nucleotide Archive) API for `host_age`.\n",
    "\n",
    "3. **Longitudinal Labeling**: For early prediction, we label ALL samples from a patient based on whether they **ever develop** a food allergy (not just their status at that timepoint).\n",
    "\n",
    "## Output\n",
    "\n",
    "- `diabimmune_longitudinal_labels.csv` — Full dataset with longitudinal labels\n",
    "- `preprocessed_diabimmune_longitudinal/Month_X.csv` — Per-month files for training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c6b8a3-4f3e-4192-be99-183bc27b922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (4.13.5)\n",
      "Requirement already satisfied: pandas in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting atlasclient\n",
      "  Downloading atlasclient-1.0.0.tar.gz (46 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from atlasclient) (1.17.0)\n",
      "Requirement already satisfied: Click>=6.0 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from atlasclient) (8.2.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from atlasclient) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests>=2.18.4->atlasclient) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests>=2.18.4->atlasclient) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests>=2.18.4->atlasclient) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vaibhavpandey/.pyenv/versions/ml-dev/lib/python3.11/site-packages (from requests>=2.18.4->atlasclient) (2025.8.3)\n",
      "Building wheels for collected packages: atlasclient\n",
      "\u001b[33m  DEPRECATION: Building 'atlasclient' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'atlasclient'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for atlasclient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for atlasclient: filename=atlasclient-1.0.0-py3-none-any.whl size=29174 sha256=ab13d3076d4e94f0a62a8bdca566f4c49c4b5ce199520a6b4bbe41b2b901c421\n",
      "  Stored in directory: /Users/vaibhavpandey/Library/Caches/pip/wheels/b2/77/c6/94d6fd4ca781735e546114b844f6f5a25d592664fd305e9096\n",
      "Successfully built atlasclient\n",
      "Installing collected packages: atlasclient\n",
      "Successfully installed atlasclient-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas\n",
    "!pip install atlasclient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce235f5",
   "metadata": {},
   "source": [
    "## Step 1: Load Source Data\n",
    "\n",
    "We have two main data sources:\n",
    "\n",
    "1. **`download_samples_tsv-2.tsv`** — SRS IDs from MicrobeAtlas (public microbiome database)\n",
    "2. **`metadata.csv`** — DIABIMMUNE study metadata with patient IDs, collection months, and allergy data\n",
    "\n",
    "**Problem**: These don't share a common key! The SRS file has no `subjectID` or `collection_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb491a47-113c-49b1-b28c-b2f2db1e9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS columns: ['SRS', 'name', 'note', 'sample_env', 'keywords_clean', 'taxa_stats', 'num_rids', 'num_hq_runs', 'rids', 'projects', 'publications']\n",
      "Meta columns: ['subjectID', 'SampleID', 'age_at_collection', 'collection_month', 'delivery', 'gest_time', 'gender', 'country', 'Exclusive_breast_feeding', 'Breast_feeding_end', 'Regular_formula', 'hydrosylated_formula', 'partly_hydrosylated_formula', 'Any_baby_formula', 'Fruits_and_berries', 'Corn', 'Rice', 'Wheat', 'Oat', 'Barley', 'Rye', 'Cereal', 'Other_grains', 'Root_vegetables', 'Vegetables', 'Eggs', 'Soy', 'Milk', 'Meat', 'Fish', 'Other_food', 'Other_than_BF', 'bf_length', 'num_abx_treatments', 'num_abx_first_year', 'abx_first_year', 'after_abx', 'num_preceeding_abx', 'hla_risk_class', 'seroconverted', 'num_aabs', 'totalige', 'totalige_log', 'allergy_milk', 'allergy_egg', 'allergy_peanut', 'allergy_dustmite', 'totalige_high', 'allergy_cat', 'allergy_dog', 'allergy_birch', 'allergy_timothy', 'gid_wgs', 'mgx_reads', 'mgx_pool', 'mgx_reads_filtered', 'read_count_16S', 'sequencing_PDO_16S', 'gid_16s']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_srs = pd.read_csv(\"download_samples_tsv-2.tsv\", sep=\"\\t\", dtype=str)\n",
    "df_srs = df_srs.rename(columns={\"#sid\": \"SRS\"})\n",
    "df_srs[\"SRS\"] = df_srs[\"SRS\"].astype(str)\n",
    "\n",
    "df_meta = pd.read_csv(\"metadata.csv\", dtype=str)\n",
    "\n",
    "print(\"SRS columns:\", df_srs.columns.tolist())\n",
    "print(\"Meta columns:\", df_meta.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3a393",
   "metadata": {},
   "source": [
    "## Step 2: Link SRS to Patient IDs via ENA API\n",
    "\n",
    "**Solution**: Each SRS accession has metadata stored in the ENA (European Nucleotide Archive). We can query it to extract `host_subject_id` (the patient ID).\n",
    "\n",
    "The function below queries the ENA XML API and parses the `host_subject_id` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a3e447-acf7-4c11-896a-2b978b7c9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_host_subject_id(srs_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use ENA browser XML API to get host_subject_id for a given SRS accession.\n",
    "    This matches the 'Original MetaData' you see in MicrobeAtlas.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.ebi.ac.uk/ena/browser/api/xml/{srs_id}\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] SRS={srs_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    xml = r.text\n",
    "\n",
    "    # ENA attribute structure is like:\n",
    "    # <SAMPLE_ATTRIBUTE>\n",
    "    #   <TAG>host_subject_id</TAG>\n",
    "    #   <VALUE>P018832</VALUE>\n",
    "    # </SAMPLE_ATTRIBUTE>\n",
    "    m = re.search(\n",
    "        r\"<TAG>\\s*host_subject_id\\s*</TAG>\\s*<VALUE>\\s*([^<\\s]+)\\s*</VALUE>\",\n",
    "        xml,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    print(f\"[WARN] no host_subject_id found in XML for {srs_id}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea88542-115c-425b-ac2b-2ac9868f01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SRS: SRS1719087\n",
      "host_subject_id: P018832\n"
     ]
    }
   ],
   "source": [
    "test_srs = \"SRS1719087\"\n",
    "print(\"Testing SRS:\", test_srs)\n",
    "print(\"host_subject_id:\", get_host_subject_id(test_srs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db8e6fb-0f61-47af-be66-9621deca9a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique SRS: 785\n",
      "50/785 SRS processed\n",
      "100/785 SRS processed\n",
      "150/785 SRS processed\n",
      "200/785 SRS processed\n",
      "250/785 SRS processed\n",
      "300/785 SRS processed\n",
      "350/785 SRS processed\n",
      "400/785 SRS processed\n",
      "450/785 SRS processed\n",
      "500/785 SRS processed\n",
      "550/785 SRS processed\n",
      "600/785 SRS processed\n",
      "650/785 SRS processed\n",
      "700/785 SRS processed\n",
      "750/785 SRS processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRS</th>\n",
       "      <th>host_subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRS1719087</td>\n",
       "      <td>P018832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRS1719088</td>\n",
       "      <td>P017743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRS1719089</td>\n",
       "      <td>P000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SRS1719090</td>\n",
       "      <td>T022883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SRS1719091</td>\n",
       "      <td>T012374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SRS host_subject_id\n",
       "0  SRS1719087         P018832\n",
       "1  SRS1719088         P017743\n",
       "2  SRS1719089         P000648\n",
       "3  SRS1719090         T022883\n",
       "4  SRS1719091         T012374"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "df_srs = pd.read_csv(\"download_samples_tsv-2.tsv\", sep=\"\\t\", dtype=str)\n",
    "df_srs = df_srs.rename(columns={\"#sid\": \"SRS\"})\n",
    "df_srs[\"SRS\"] = df_srs[\"SRS\"].astype(str)\n",
    "\n",
    "unique_srs = df_srs[\"SRS\"].dropna().unique().tolist()\n",
    "print(\"Number of unique SRS:\", len(unique_srs))\n",
    "\n",
    "rows = []\n",
    "for i, srs in enumerate(unique_srs, start=1):\n",
    "    subj = get_host_subject_id(srs)\n",
    "    rows.append({\"SRS\": srs, \"host_subject_id\": subj})\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"{i}/{len(unique_srs)} SRS processed\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "df_map = pd.DataFrame(rows)\n",
    "df_map.to_csv(\"srs_to_host_subject_id.csv\", index=False)\n",
    "df_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaed3ed",
   "metadata": {},
   "source": [
    "## Step 3: Extract Collection Month from ENA\n",
    "\n",
    "**Problem**: Even with `subjectID`, we can't merge directly because one patient has multiple samples across different months. Merging only on `subjectID` creates a **Cartesian product** (data leakage!).\n",
    "\n",
    "**Solution**: The ENA metadata includes `host_age` (age in days at sample collection). We convert this to `collection_month`:\n",
    "\n",
    "```\n",
    "collection_month = round(host_age_days / 30.44)\n",
    "```\n",
    "\n",
    "This gives us the **SRS → collection_month** mapping we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32ea9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "srs_to_collection_month.csv already exists. Loading instead of extracting...\n",
      "✓ Loaded 785 SRS samples from srs_to_collection_month.csv\n",
      "          SRS host_age_days collection_month host_subject_id_ena\n",
      "0  SRS1719087           686               23             P018832\n",
      "1  SRS1719088           173                6             P017743\n",
      "2  SRS1719089           493               16             P000648\n",
      "3  SRS1719090           229                8             T022883\n",
      "4  SRS1719091           502               16             T012374\n",
      "5  SRS1719092           390               13             T016811\n",
      "6  SRS1719093           427               14             T017394\n",
      "7  SRS1719094           587               19             T007750\n",
      "8  SRS1719095           598               20             T003950\n",
      "9  SRS1719096           594               20             T004341\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "srs_month_csv = \"srs_to_collection_month.csv\"\n",
    "\n",
    "if os.path.exists(srs_month_csv):\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{srs_month_csv} already exists. Loading instead of extracting...\")\n",
    "    df_srs_month = pd.read_csv(srs_month_csv, dtype=str)\n",
    "    print(f\"✓ Loaded {len(df_srs_month)} SRS samples from {srs_month_csv}\")\n",
    "    print(df_srs_month.head(10))\n",
    "else:\n",
    "    # SOLUTION: Extract host_age for all SRS samples and convert to collection_month\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXTRACTING host_age FOR ALL SRS SAMPLES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Extract for all unique SRS\n",
    "    unique_srs = df_map['SRS'].dropna().unique().tolist()\n",
    "    print(f\"\\nProcessing {len(unique_srs)} SRS samples...\")\n",
    "\n",
    "    srs_age_data = []\n",
    "    failed_srs = []\n",
    "\n",
    "    for i, srs in enumerate(unique_srs, start=1):\n",
    "        metadata = get_sra_metadata(srs)\n",
    "        \n",
    "        if metadata and 'host_age' in metadata:\n",
    "            try:\n",
    "                age_days = int(metadata['host_age'])\n",
    "                # Convert days to months (using 30.44 days/month average)\n",
    "                age_months = round(age_days / 30.44)\n",
    "                \n",
    "                srs_age_data.append({\n",
    "                    'SRS': srs,\n",
    "                    'host_age_days': age_days,\n",
    "                    'collection_month': age_months,\n",
    "                    'host_subject_id_ena': metadata.get('host_subject_id', None)\n",
    "                })\n",
    "            except ValueError:\n",
    "                print(f\"  [{i}] {srs}: Invalid host_age value: {metadata['host_age']}\")\n",
    "                failed_srs.append(srs)\n",
    "        else:\n",
    "            print(f\"  [{i}] {srs}: No host_age found\")\n",
    "            failed_srs.append(srs)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Processed {i}/{len(unique_srs)} samples...\")\n",
    "            time.sleep(0.5)  # Be nice to the API\n",
    "\n",
    "    print(f\"\\n✓ Successfully extracted {len(srs_age_data)} samples\")\n",
    "    print(f\"✗ Failed for {len(failed_srs)} samples\")\n",
    "\n",
    "    # Create DataFrame with SRS → collection_month mapping\n",
    "    df_srs_month = pd.DataFrame(srs_age_data)\n",
    "    df_srs_month.to_csv(srs_month_csv, index=False)\n",
    "    print(f\"\\nSaved to: {srs_month_csv}\")\n",
    "    print(df_srs_month.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759aadd",
   "metadata": {},
   "source": [
    "## Step 4: Correct Merge on (subjectID, collection_month)\n",
    "\n",
    "Now we can perform a **correct merge** using BOTH `subjectID` AND `collection_month` as keys. This ensures:\n",
    "\n",
    "- Each SRS maps to exactly one timepoint\n",
    "- No data leakage between timepoints\n",
    "- Allergy labels correspond to the correct sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9007aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERFORMING CORRECT MERGE WITH COLLECTION MONTH\n",
      "======================================================================\n",
      "✓ Loaded 785 SRS samples with collection months\n",
      "✓ Loaded 1946 metadata rows\n",
      "\n",
      "Performing merge on ['subjectID', 'collection_month']...\n",
      "\n",
      "✓ Merge complete: 826 rows\n",
      "  (Should be same as number of SRS samples: 785)\n",
      "\n",
      "⚠ WARNING: 40 SRS samples still have duplicates after merge!\n",
      "First few duplicates:\n",
      "SRS\n",
      "SRS1719147    2\n",
      "SRS1719148    2\n",
      "SRS1719152    3\n",
      "SRS1719166    2\n",
      "SRS1719199    2\n",
      "SRS1719227    2\n",
      "SRS1719237    2\n",
      "SRS1719283    2\n",
      "SRS1719304    2\n",
      "SRS1719316    2\n",
      "dtype: int64\n",
      "\n",
      "Sample merged data:\n",
      "          SRS subjectID country  collection_month allergy_milk allergy_egg  \\\n",
      "0  SRS1719087   P018832     RUS                23        False       False   \n",
      "1  SRS1719088   P017743     RUS                 6          NaN         NaN   \n",
      "2  SRS1719089   P000648     RUS                16        False       False   \n",
      "3  SRS1719090   T022883     EST                 8         True        True   \n",
      "4  SRS1719091   T012374     EST                16        False       False   \n",
      "5  SRS1719092   T016811     EST                13         True        True   \n",
      "6  SRS1719093   T017394     EST                14        False       False   \n",
      "7  SRS1719094   T007750     EST                19        False       False   \n",
      "8  SRS1719095   T003950     EST                20         True       False   \n",
      "9  SRS1719096   T004341     EST                20        False       False   \n",
      "\n",
      "  allergy_peanut  \n",
      "0          False  \n",
      "1            NaN  \n",
      "2          False  \n",
      "3          False  \n",
      "4          False  \n",
      "5           True  \n",
      "6          False  \n",
      "7          False  \n",
      "8          False  \n",
      "9          False  \n"
     ]
    }
   ],
   "source": [
    "# CORRECT MERGE: Use SRS → collection_month mapping\n",
    "print(\"=\"*70)\n",
    "print(\"PERFORMING CORRECT MERGE WITH COLLECTION MONTH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the extracted SRS → collection_month mapping\n",
    "df_srs_month = pd.read_csv(\"srs_to_collection_month.csv\", dtype=str)\n",
    "df_srs_month['collection_month'] = pd.to_numeric(df_srs_month['collection_month'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Load metadata\n",
    "df_meta = pd.read_csv(\"metadata.csv\", dtype=str)\n",
    "df_meta['collection_month'] = pd.to_numeric(df_meta['collection_month'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Detect subject column in metadata\n",
    "subj_col = None\n",
    "for c in df_meta.columns:\n",
    "    if c.lower() in (\"subjectid\", \"subject_id\", \"host_subject_id\", \"host_subjectid\"):\n",
    "        subj_col = c\n",
    "        break\n",
    "\n",
    "if subj_col:\n",
    "    df_meta = df_meta.rename(columns={subj_col: \"subjectID\"})\n",
    "    df_srs_month = df_srs_month.rename(columns={\"host_subject_id_ena\": \"subjectID\"})\n",
    "\n",
    "print(f\"✓ Loaded {len(df_srs_month)} SRS samples with collection months\")\n",
    "print(f\"✓ Loaded {len(df_meta)} metadata rows\")\n",
    "\n",
    "# CORRECT MERGE: on BOTH subjectID AND collection_month\n",
    "print(\"\\nPerforming merge on ['subjectID', 'collection_month']...\")\n",
    "df_merged_correct = df_srs_month.merge(\n",
    "    df_meta, \n",
    "    on=['subjectID', 'collection_month'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merge complete: {len(df_merged_correct)} rows\")\n",
    "print(f\"  (Should be same as number of SRS samples: {len(df_srs_month)})\")\n",
    "\n",
    "# Check for any duplicates\n",
    "duplicate_check = df_merged_correct.groupby('SRS').size()\n",
    "duplicates = duplicate_check[duplicate_check > 1]\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"\\n⚠ WARNING: {len(duplicates)} SRS samples still have duplicates after merge!\")\n",
    "    print(\"First few duplicates:\")\n",
    "    print(duplicates.head(10))\n",
    "else:\n",
    "    print(\"\\n✓ SUCCESS: No sample appears in multiple rows!\")\n",
    "\n",
    "# Show sample of merged data\n",
    "print(\"\\nSample merged data:\")\n",
    "print(df_merged_correct[['SRS', 'subjectID', 'country', 'collection_month', 'allergy_milk', 'allergy_egg', 'allergy_peanut']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415c7d8",
   "metadata": {},
   "source": [
    "## Step 5: Handle Duplicate Rows\n",
    "\n",
    "Some patients have **multiple physical samples** (different `SampleID`s) collected in the same month. After the merge, these create duplicate rows for the same SRS.\n",
    "\n",
    "**Solution**: Aggregate by SRS, taking the maximum allergy value (if ANY sample shows allergy, mark as allergic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b38eee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INVESTIGATING DUPLICATES\n",
      "======================================================================\n",
      "\n",
      "40 SRS samples have duplicates\n",
      "\n",
      "Example: SRS1719152 (has 3 rows)\n",
      "           SRS subjectID country  collection_month SampleID allergy_milk  \\\n",
      "67  SRS1719152   P020604     RUS                13  3104057          NaN   \n",
      "68  SRS1719152   P020604     RUS                13  3104056          NaN   \n",
      "69  SRS1719152   P020604     RUS                13  3104053          NaN   \n",
      "\n",
      "   allergy_egg allergy_peanut  \n",
      "67         NaN            NaN  \n",
      "68         NaN            NaN  \n",
      "69         NaN            NaN  \n",
      "\n",
      "REASON: Some patients have multiple physical samples (different SampleIDs) collected in the same month.\n",
      "SOLUTION: Aggregate allergy data - if ANY sample in that month shows allergy, mark as allergic.\n",
      "\n",
      "Aggregating duplicate rows...\n",
      "✓ After aggregation: 785 rows (one per SRS)\n",
      "✓ No duplicates: True\n",
      "\n",
      "Final data sample:\n",
      "          SRS subjectID country  collection_month allergy_milk allergy_egg  \\\n",
      "0  SRS1719087   P018832     RUS                23        False       False   \n",
      "1  SRS1719088   P017743     RUS                 6        False       False   \n",
      "2  SRS1719089   P000648     RUS                16        False       False   \n",
      "3  SRS1719090   T022883     EST                 8         True        True   \n",
      "4  SRS1719091   T012374     EST                16        False       False   \n",
      "5  SRS1719092   T016811     EST                13         True        True   \n",
      "6  SRS1719093   T017394     EST                14        False       False   \n",
      "7  SRS1719094   T007750     EST                19        False       False   \n",
      "8  SRS1719095   T003950     EST                20         True       False   \n",
      "9  SRS1719096   T004341     EST                20        False       False   \n",
      "\n",
      "  allergy_peanut  \n",
      "0          False  \n",
      "1          False  \n",
      "2          False  \n",
      "3          False  \n",
      "4          False  \n",
      "5           True  \n",
      "6          False  \n",
      "7          False  \n",
      "8          False  \n",
      "9          False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/bjm8mgw92d53x5w4s4hbgbww0000gn/T/ipykernel_97937/3439728408.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_merged_correct.groupby('SRS', as_index=False).apply(aggregate_allergy_data)\n"
     ]
    }
   ],
   "source": [
    "# Investigate and resolve duplicates\n",
    "print(\"=\"*70)\n",
    "print(\"INVESTIGATING DUPLICATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find which SRS have duplicates\n",
    "duplicate_srs = df_merged_correct.groupby('SRS').size()\n",
    "duplicate_srs = duplicate_srs[duplicate_srs > 1].index.tolist()\n",
    "\n",
    "print(f\"\\n{len(duplicate_srs)} SRS samples have duplicates\")\n",
    "print(\"\\nExample: SRS1719152 (has 3 rows)\")\n",
    "example = df_merged_correct[df_merged_correct['SRS'] == 'SRS1719152'][['SRS', 'subjectID', 'country', 'collection_month', 'SampleID', 'allergy_milk', 'allergy_egg', 'allergy_peanut']]\n",
    "print(example)\n",
    "\n",
    "print(\"\\nREASON: Some patients have multiple physical samples (different SampleIDs) collected in the same month.\")\n",
    "print(\"SOLUTION: Aggregate allergy data - if ANY sample in that month shows allergy, mark as allergic.\")\n",
    "\n",
    "# Aggregate: for each SRS, take max of allergy values (True > False)\n",
    "# This ensures if any sample that month was allergic, we mark it as allergic\n",
    "\n",
    "def aggregate_allergy_data(group):\n",
    "    \"\"\"For duplicate rows, aggregate allergy info\"\"\"\n",
    "    result = group.iloc[0].copy()  # Start with first row\n",
    "    \n",
    "    # For allergy columns, take the max (True > False)\n",
    "    allergy_cols = ['allergy_milk', 'allergy_egg', 'allergy_peanut', \n",
    "                    'allergy_dustmite', 'totalige_high', 'allergy_cat', \n",
    "                    'allergy_dog', 'allergy_birch', 'allergy_timothy']\n",
    "    \n",
    "    for col in allergy_cols:\n",
    "        if col in group.columns:\n",
    "            # Convert to boolean, taking True if any row is True\n",
    "            vals = group[col].fillna('False').astype(str).str.lower()\n",
    "            result[col] = 'True' if any(v in {'1', 'true', 'yes'} for v in vals) else 'False'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\nAggregating duplicate rows...\")\n",
    "df_final = df_merged_correct.groupby('SRS', as_index=False).apply(aggregate_allergy_data)\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ After aggregation: {len(df_final)} rows (one per SRS)\")\n",
    "print(f\"✓ No duplicates: {df_final['SRS'].nunique() == len(df_final)}\")\n",
    "\n",
    "print(\"\\nFinal data sample:\")\n",
    "print(df_final[['SRS', 'subjectID', 'country', 'collection_month', 'allergy_milk', 'allergy_egg', 'allergy_peanut']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24233ac9",
   "metadata": {},
   "source": [
    "## Step 6: Longitudinal Labeling for Early Prediction\n",
    "\n",
    "**This is the key step for our prediction task!**\n",
    "\n",
    "For **early allergy prediction**, we want to identify patients who will **eventually develop** an allergy, even from samples collected before the allergy manifests.\n",
    "\n",
    "**Strategy**: For each patient, compute their **maximum/latest** allergy status across ALL timepoints. Then apply this label to ALL their samples.\n",
    "\n",
    "| Patient | Month 4 Status | Month 10 Status | Label Applied |\n",
    "|---------|----------------|-----------------|---------------|\n",
    "| T022883 | Not allergic   | Milk allergy    | **Allergic** (all samples) |\n",
    "\n",
    "This enables the model to learn early biomarkers that predict future allergy development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "352dc341-0748-48e1-b430-0650836a241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "APPLYING LONGITUDINAL LABELING STRATEGY\n",
      "======================================================================\n",
      "\n",
      "Step 1: Computing maximum allergy status per patient...\n",
      "✓ Computed max allergy status for 212 patients\n",
      "\n",
      "Sample patient max allergy status:\n",
      "  subjectID  allergy_milk  allergy_egg  allergy_peanut\n",
      "0   E002338          True        False           False\n",
      "1   E002473         False        False           False\n",
      "2   E002681         False        False           False\n",
      "3   E002825          True        False           False\n",
      "4   E003393         False        False           False\n",
      "5   E004071         False        False           False\n",
      "6   E004080         False        False           False\n",
      "7   E004781          True         True           False\n",
      "8   E004934         False         True           False\n",
      "9   E005804         False        False           False\n",
      "\n",
      "Step 2: Applying longitudinal labels to all samples...\n",
      "✓ Labeled 785 samples\n",
      "\n",
      "Sample labeled data:\n",
      "           SRS subjectID country  collection_month  allergy_milk  allergy_egg  \\\n",
      "0   SRS1719087   P018832     RUS                23         False        False   \n",
      "1   SRS1719088   P017743     RUS                 6         False        False   \n",
      "2   SRS1719089   P000648     RUS                16         False        False   \n",
      "3   SRS1719090   T022883     EST                 8          True         True   \n",
      "4   SRS1719091   T012374     EST                16         False        False   \n",
      "5   SRS1719092   T016811     EST                13          True         True   \n",
      "6   SRS1719093   T017394     EST                14         False        False   \n",
      "7   SRS1719094   T007750     EST                19         False        False   \n",
      "8   SRS1719095   T003950     EST                20          True        False   \n",
      "9   SRS1719096   T004341     EST                20         False        False   \n",
      "10  SRS1719097   T012374     EST                13         False        False   \n",
      "11  SRS1719098   T012977     EST                16         False        False   \n",
      "12  SRS1719099   T017419     EST                13         False        False   \n",
      "13  SRS1719100   P015091     RUS                 9         False         True   \n",
      "14  SRS1719101   T016811     EST                16          True         True   \n",
      "15  SRS1719102   T008246     EST                16          True        False   \n",
      "16  SRS1719103   T008025     EST                22          True        False   \n",
      "17  SRS1719104   T017394     EST                16         False        False   \n",
      "18  SRS1719105   E016444     FIN                13         False        False   \n",
      "19  SRS1719106   E007303     FIN                21         False        False   \n",
      "\n",
      "    allergy_peanut  label  allergen_class  \n",
      "0            False      0               0  \n",
      "1            False      0               0  \n",
      "2            False      0               0  \n",
      "3            False      1               4  \n",
      "4            False      0               0  \n",
      "5             True      1               4  \n",
      "6            False      0               0  \n",
      "7            False      0               0  \n",
      "8            False      1               1  \n",
      "9            False      0               0  \n",
      "10           False      0               0  \n",
      "11           False      0               0  \n",
      "12           False      0               0  \n",
      "13           False      1               2  \n",
      "14            True      1               4  \n",
      "15           False      1               1  \n",
      "16           False      1               1  \n",
      "17           False      0               0  \n",
      "18           False      0               0  \n",
      "19           False      0               0  \n"
     ]
    }
   ],
   "source": [
    "# LONGITUDINAL LABELING: Option 3 - Latest/Maximum allergy status per patient\n",
    "print(\"=\"*70)\n",
    "print(\"APPLYING LONGITUDINAL LABELING STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Food allergy columns\n",
    "food_allergy_cols = [\n",
    "    \"allergy_milk\",\n",
    "    \"allergy_egg\",\n",
    "    \"allergy_peanut\",\n",
    "]\n",
    "\n",
    "def is_allergic(val):\n",
    "    \"\"\"Check if a value represents allergic status\"\"\"\n",
    "    v = str(val).strip().lower()\n",
    "    return v in {\"1\", \"true\", \"yes\"}\n",
    "\n",
    "# For each patient, compute their MAXIMUM/LATEST allergy status across all timepoints\n",
    "print(\"\\nStep 1: Computing maximum allergy status per patient...\")\n",
    "\n",
    "patient_max_allergy = df_final.groupby('subjectID').agg({\n",
    "    'allergy_milk': lambda x: any(is_allergic(v) for v in x),\n",
    "    'allergy_egg': lambda x: any(is_allergic(v) for v in x),\n",
    "    'allergy_peanut': lambda x: any(is_allergic(v) for v in x)\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"✓ Computed max allergy status for {len(patient_max_allergy)} patients\")\n",
    "print(\"\\nSample patient max allergy status:\")\n",
    "print(patient_max_allergy.head(10))\n",
    "\n",
    "# Apply longitudinal labels: all samples from a patient get labeled with their max allergy\n",
    "print(\"\\nStep 2: Applying longitudinal labels to all samples...\")\n",
    "\n",
    "df_longitudinal = df_final.drop(columns=food_allergy_cols).merge(\n",
    "    patient_max_allergy, \n",
    "    on='subjectID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create binary label and allergen class\n",
    "def get_food_allergy_label(row):\n",
    "    \"\"\"Binary label: 1 if any food allergy, 0 otherwise\"\"\"\n",
    "    return 1 if any([row['allergy_milk'], row['allergy_egg'], row['allergy_peanut']]) else 0\n",
    "\n",
    "def get_allergen_class(row):\n",
    "    \"\"\"\n",
    "    Allergen class label:\n",
    "    0 = non-allergic\n",
    "    1 = milk allergy only\n",
    "    2 = egg allergy only\n",
    "    3 = peanut allergy only\n",
    "    4 = multiple food allergies\n",
    "    \"\"\"\n",
    "    allergies = []\n",
    "    if row['allergy_milk']:\n",
    "        allergies.append(1)\n",
    "    if row['allergy_egg']:\n",
    "        allergies.append(2)\n",
    "    if row['allergy_peanut']:\n",
    "        allergies.append(3)\n",
    "    \n",
    "    if len(allergies) == 0:\n",
    "        return 0\n",
    "    elif len(allergies) == 1:\n",
    "        return allergies[0]\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "df_longitudinal[\"label\"] = df_longitudinal.apply(get_food_allergy_label, axis=1)\n",
    "df_longitudinal[\"allergen_class\"] = df_longitudinal.apply(get_allergen_class, axis=1)\n",
    "\n",
    "print(f\"✓ Labeled {len(df_longitudinal)} samples\")\n",
    "print(\"\\nSample labeled data:\")\n",
    "print(df_longitudinal[['SRS', 'subjectID', 'country', 'collection_month', 'allergy_milk', 'allergy_egg', 'allergy_peanut', 'label', 'allergen_class']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05ff88",
   "metadata": {},
   "source": [
    "## Step 7: Verification\n",
    "\n",
    "Let's verify the longitudinal labeling is working correctly by checking a patient who develops an allergy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7ae344-b04c-4399-b764-1bb9a02f1178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICATION: Checking longitudinal labeling\n",
      "======================================================================\n",
      "\n",
      "Example: Patient T022883 (develops milk allergy)\n",
      "            SRS country  collection_month  allergy_milk  allergy_egg  \\\n",
      "535  SRS1735472     EST                 4          True         True   \n",
      "3    SRS1719090     EST                 8          True         True   \n",
      "444  SRS1719531     EST                10          True         True   \n",
      "92   SRS1719179     EST                13          True         True   \n",
      "113  SRS1719200     EST                16          True         True   \n",
      "\n",
      "     allergy_peanut  label  \n",
      "535           False      1  \n",
      "3             False      1  \n",
      "444           False      1  \n",
      "92            False      1  \n",
      "113           False      1  \n",
      "\n",
      "✓ All samples from this patient are labeled as allergic (label=1)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "Total samples: 785\n",
      "Unique patients: 212\n",
      "Collection months range: 1 - 38\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    527\n",
      "1    258\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Allergen class distribution:\n",
      "allergen_class\n",
      "0    527\n",
      "1    101\n",
      "2     63\n",
      "3      7\n",
      "4     87\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify longitudinal labeling\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION: Checking longitudinal labeling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example: Check a patient who develops allergy later\n",
    "# Find a patient with milk allergy\n",
    "allergic_patient = df_longitudinal[df_longitudinal['allergy_milk'] == True]['subjectID'].iloc[0] if any(df_longitudinal['allergy_milk'] == True) else None\n",
    "\n",
    "if allergic_patient:\n",
    "    print(f\"\\nExample: Patient {allergic_patient} (develops milk allergy)\")\n",
    "    patient_samples = df_longitudinal[df_longitudinal['subjectID'] == allergic_patient][\n",
    "        ['SRS', 'country', 'collection_month', 'allergy_milk', 'allergy_egg', 'allergy_peanut', 'label']\n",
    "    ].sort_values('collection_month')\n",
    "    print(patient_samples)\n",
    "    print(\"\\n✓ All samples from this patient are labeled as allergic (label=1)\")\n",
    "else:\n",
    "    print(\"\\nNo allergic patients found in dataset\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(df_longitudinal)}\")\n",
    "print(f\"Unique patients: {df_longitudinal['subjectID'].nunique()}\")\n",
    "print(f\"Collection months range: {df_longitudinal['collection_month'].min()} - {df_longitudinal['collection_month'].max()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_longitudinal['label'].value_counts().sort_index())\n",
    "print(f\"\\nAllergen class distribution:\")\n",
    "print(df_longitudinal['allergen_class'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef1f7e",
   "metadata": {},
   "source": [
    "## Step 7.5: Impute Missing Country Information\n",
    "\n",
    "Some samples may have missing country data. Since a patient's country doesn't change over time, we can impute missing country values using the available country information from other samples of the same patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bee301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMPUTING MISSING COUNTRY VALUES\n",
      "======================================================================\n",
      "\n",
      "Before imputation: 1 samples with missing country\n",
      "\n",
      "Samples with missing country:\n",
      "            SRS subjectID country  collection_month\n",
      "501  SRS1735438   P005558     NaN                36\n",
      "\n",
      "✓ Imputed 1 country values\n",
      "After imputation: 0 samples still missing country\n",
      "\n",
      "Samples after imputation:\n",
      "            SRS subjectID country  collection_month\n",
      "501  SRS1735438   P005558     RUS                36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/bjm8mgw92d53x5w4s4hbgbww0000gn/T/ipykernel_97937/2638573877.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_longitudinal = df_longitudinal.groupby('subjectID', group_keys=False).apply(impute_country)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"IMPUTING MISSING COUNTRY VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for missing country values\n",
    "missing_country_before = df_longitudinal['country'].isna().sum()\n",
    "print(f\"\\nBefore imputation: {missing_country_before} samples with missing country\")\n",
    "\n",
    "if missing_country_before > 0:\n",
    "    # Show samples with missing country\n",
    "    missing_samples = df_longitudinal[df_longitudinal['country'].isna()][['SRS', 'subjectID', 'country', 'collection_month']]\n",
    "    print(f\"\\nSamples with missing country:\")\n",
    "    print(missing_samples)\n",
    "    \n",
    "    # For each patient, fill missing country with the most common country value for that patient\n",
    "    def impute_country(group):\n",
    "        \"\"\"Fill missing country values within a patient group\"\"\"\n",
    "        # Get non-null country values for this patient\n",
    "        valid_countries = group['country'].dropna()\n",
    "        \n",
    "        if len(valid_countries) > 0:\n",
    "            # Use the most common country (should be unique per patient, but just in case)\n",
    "            most_common_country = valid_countries.mode()\n",
    "            if len(most_common_country) > 0:\n",
    "                group['country'] = group['country'].fillna(most_common_country.iloc[0])\n",
    "        \n",
    "        return group\n",
    "    \n",
    "    # Apply imputation by patient\n",
    "    df_longitudinal = df_longitudinal.groupby('subjectID', group_keys=False).apply(impute_country)\n",
    "    \n",
    "    # Check results\n",
    "    missing_country_after = df_longitudinal['country'].isna().sum()\n",
    "    imputed_count = missing_country_before - missing_country_after\n",
    "    \n",
    "    print(f\"\\n✓ Imputed {imputed_count} country values\")\n",
    "    print(f\"After imputation: {missing_country_after} samples still missing country\")\n",
    "    \n",
    "    if imputed_count > 0:\n",
    "        # Show what was imputed\n",
    "        imputed_samples = df_longitudinal[df_longitudinal['SRS'].isin(missing_samples['SRS'])][['SRS', 'subjectID', 'country', 'collection_month']]\n",
    "        print(f\"\\nSamples after imputation:\")\n",
    "        print(imputed_samples)\n",
    "else:\n",
    "    print(\"\\n✓ No missing country values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e1446",
   "metadata": {},
   "source": [
    "## Step 8: Save Output Files\n",
    "\n",
    "Save the final preprocessed dataset with longitudinal labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f139db0-49d5-4395-a011-23d15f338080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "✓ Saved longitudinally-labeled dataset to: diabimmune_longitudinal_labels.csv\n",
      "======================================================================\n",
      "Columns: ['SRS', 'host_age_days', 'collection_month', 'subjectID', 'SampleID', 'age_at_collection', 'delivery', 'gest_time', 'gender', 'country', 'Exclusive_breast_feeding', 'Breast_feeding_end', 'Regular_formula', 'hydrosylated_formula', 'partly_hydrosylated_formula', 'Any_baby_formula', 'Fruits_and_berries', 'Corn', 'Rice', 'Wheat', 'Oat', 'Barley', 'Rye', 'Cereal', 'Other_grains', 'Root_vegetables', 'Vegetables', 'Eggs', 'Soy', 'Milk', 'Meat', 'Fish', 'Other_food', 'Other_than_BF', 'bf_length', 'num_abx_treatments', 'num_abx_first_year', 'abx_first_year', 'after_abx', 'num_preceeding_abx', 'hla_risk_class', 'seroconverted', 'num_aabs', 'totalige', 'totalige_log', 'allergy_dustmite', 'totalige_high', 'allergy_cat', 'allergy_dog', 'allergy_birch', 'allergy_timothy', 'gid_wgs', 'mgx_reads', 'mgx_pool', 'mgx_reads_filtered', 'read_count_16S', 'sequencing_PDO_16S', 'gid_16s', 'allergy_milk', 'allergy_egg', 'allergy_peanut', 'label', 'allergen_class']\n",
      "\n",
      "First 10 rows:\n",
      "          SRS subjectID country  collection_month  label  allergen_class\n",
      "0  SRS1719087   P018832     RUS                23      0               0\n",
      "1  SRS1719088   P017743     RUS                 6      0               0\n",
      "2  SRS1719089   P000648     RUS                16      0               0\n",
      "3  SRS1719090   T022883     EST                 8      1               4\n",
      "4  SRS1719091   T012374     EST                16      0               0\n",
      "5  SRS1719092   T016811     EST                13      1               4\n",
      "6  SRS1719093   T017394     EST                14      0               0\n",
      "7  SRS1719094   T007750     EST                19      0               0\n",
      "8  SRS1719095   T003950     EST                20      1               1\n",
      "9  SRS1719096   T004341     EST                20      0               0\n"
     ]
    }
   ],
   "source": [
    "# Save the longitudinally-labeled dataset\n",
    "output_file = \"diabimmune_longitudinal_labels.csv\"\n",
    "df_longitudinal.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Saved longitudinally-labeled dataset to: {output_file}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Columns: {df_longitudinal.columns.tolist()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(df_longitudinal[['SRS', 'subjectID', 'country', 'collection_month', 'label', 'allergen_class']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf11f7",
   "metadata": {},
   "source": [
    "## Step 9: Create Per-Month Files\n",
    "\n",
    "Generate individual CSV files for each collection month. These are useful for:\n",
    "- Month-specific model training\n",
    "- Temporal analysis\n",
    "- Cross-validation strategies\n",
    "\n",
    "**Important**: Each SRS appears in exactly ONE month (no leakage between files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdc86af5-2b84-479c-87a2-32a4217eca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating per-month files in: preprocessed_diabimmune_longitudinal/\n",
      "======================================================================\n",
      "✓ All 785 samples appear in exactly one month (no leakage)\n",
      "\n",
      "Month  1:  20 samples (9 allergic, 11 non-allergic)\n",
      "Month  2:  17 samples (11 allergic, 6 non-allergic)\n",
      "Month  3:   8 samples (2 allergic, 6 non-allergic)\n",
      "Month  4:  39 samples (24 allergic, 15 non-allergic)\n",
      "Month  5:  11 samples (1 allergic, 10 non-allergic)\n",
      "Month  6:  15 samples (5 allergic, 10 non-allergic)\n",
      "Month  7:  58 samples (16 allergic, 42 non-allergic)\n",
      "Month  8:  16 samples (4 allergic, 12 non-allergic)\n",
      "Month  9:  21 samples (6 allergic, 15 non-allergic)\n",
      "Month 10:  55 samples (17 allergic, 38 non-allergic)\n",
      "Month 11:  27 samples (4 allergic, 23 non-allergic)\n",
      "Month 12:  20 samples (5 allergic, 15 non-allergic)\n",
      "Month 13:  60 samples (24 allergic, 36 non-allergic)\n",
      "Month 14:  25 samples (5 allergic, 20 non-allergic)\n",
      "Month 15:  11 samples (1 allergic, 10 non-allergic)\n",
      "Month 16:  72 samples (28 allergic, 44 non-allergic)\n",
      "Month 17:  25 samples (4 allergic, 21 non-allergic)\n",
      "Month 18:  27 samples (2 allergic, 25 non-allergic)\n",
      "Month 19:  50 samples (18 allergic, 32 non-allergic)\n",
      "Month 20:  16 samples (2 allergic, 14 non-allergic)\n",
      "Month 21:   9 samples (1 allergic, 8 non-allergic)\n",
      "Month 22:  58 samples (26 allergic, 32 non-allergic)\n",
      "Month 23:  20 samples (4 allergic, 16 non-allergic)\n",
      "Month 24:   8 samples (1 allergic, 7 non-allergic)\n",
      "Month 25:   4 samples (0 allergic, 4 non-allergic)\n",
      "Month 26:   4 samples (2 allergic, 2 non-allergic)\n",
      "Month 27:   8 samples (4 allergic, 4 non-allergic)\n",
      "Month 28:  29 samples (10 allergic, 19 non-allergic)\n",
      "Month 29:   8 samples (4 allergic, 4 non-allergic)\n",
      "Month 30:   1 samples (0 allergic, 1 non-allergic)\n",
      "Month 32:   2 samples (1 allergic, 1 non-allergic)\n",
      "Month 33:   2 samples (1 allergic, 1 non-allergic)\n",
      "Month 35:   7 samples (2 allergic, 5 non-allergic)\n",
      "Month 36:  31 samples (14 allergic, 17 non-allergic)\n",
      "Month 38:   1 samples (0 allergic, 1 non-allergic)\n",
      "\n",
      "✓ Per-month files created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Optional: Create per-month files for downstream analysis\n",
    "import os\n",
    "\n",
    "output_dir = \"preprocessed_diabimmune_longitudinal\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Creating per-month files in: {output_dir}/\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sanity check: Each SRS should appear in exactly ONE month\n",
    "sample_month_counts = df_longitudinal.groupby(\"SRS\")[\"collection_month\"].nunique()\n",
    "leaked_samples = sample_month_counts[sample_month_counts > 1]\n",
    "\n",
    "if len(leaked_samples) > 0:\n",
    "    print(f\"⚠ WARNING: {len(leaked_samples)} samples appear in multiple months!\")\n",
    "    print(\"This should not happen with correct preprocessing!\")\n",
    "else:\n",
    "    print(f\"✓ All {len(df_longitudinal)} samples appear in exactly one month (no leakage)\\n\")\n",
    "\n",
    "# Write per-month files\n",
    "for month, grp in df_longitudinal.groupby(\"collection_month\"):\n",
    "    out = grp[[\"SRS\", \"subjectID\", \"country\", \"label\", \"allergen_class\"]].copy()\n",
    "    out = out.rename(columns={\"SRS\": \"sid\", \"subjectID\": \"patient_id\"})\n",
    "    \n",
    "    fname = f\"Month_{int(month)}.csv\"\n",
    "    fpath = os.path.join(output_dir, fname)\n",
    "    out.to_csv(fpath, index=False)\n",
    "    \n",
    "    # Count labels\n",
    "    allergic_count = (out['label'] == 1).sum()\n",
    "    non_allergic_count = (out['label'] == 0).sum()\n",
    "    print(f\"Month {int(month):2d}: {len(out):3d} samples ({allergic_count} allergic, {non_allergic_count} non-allergic)\")\n",
    "\n",
    "print(\"\\n✓ Per-month files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbd06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multicam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
